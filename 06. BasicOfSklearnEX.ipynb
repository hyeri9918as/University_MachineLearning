{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKlearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn 기본 구조 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data = [[0, 0], [0, 1], [1, 0], [1, 1]], train_label = [0, 1, 1, 0]\n",
      "test_data = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
      "accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# prepare data (XOR)\n",
    "train_data = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "train_label = [0, 1, 1, 0]\n",
    "test_data = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "\n",
    "# select a learning algorithm\n",
    "clf = svm.SVC(C=10, gamma=0.1) # C:오류허용도의 역수, gamma:학습data 의존도\n",
    "\n",
    "# learning\n",
    "clf.fit(train_data, train_label)\n",
    "\n",
    "# testing\n",
    "test_label = clf.predict(test_data)\n",
    "\n",
    "# results\n",
    "print(f\"train_data = {train_data}, train_label = {train_label}\")\n",
    "print(f\"test_data = {test_data}\")\n",
    "print(f\"accuracy = {accuracy_score(train_label, test_label)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Preprocessing 모듈 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### 1.  StandardSclaer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 평균을 제거하고 데이터를 단위 분산으로 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1. -1.  2.]\n",
      " [ 2.  0.  0.]\n",
      " [ 0.  1. -1.]]\n",
      "\n",
      "mean=[0. 0. 0.], std=[1. 1. 1.]\n",
      "\n",
      "[[ 0.         -1.22474487  1.33630621]\n",
      " [ 1.22474487  0.         -0.26726124]\n",
      " [-1.22474487  1.22474487 -1.06904497]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "data = np.array([[1., -1., 2.],\n",
    "                 [2., 0., 0.],\n",
    "                 [0., 1., -1.]])\n",
    "print(data)\n",
    "print(\"\")\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(data)\n",
    "scaled_data = scaler.transform(data)\n",
    "print(f\"mean={scaled_data.mean(axis=0)}, std={scaled_data.std(axis=0)}\")\n",
    "print(\"\")\n",
    "print(scaled_data)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. MinMaxScaler "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모든 feature 값이 0~1사이에 있도록 데이터를 재조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1. -1.  2.]\n",
      " [ 2.  0.  0.]\n",
      " [ 0.  1. -1.]]\n",
      "\n",
      "mean = [0.5        0.5        0.44444444], std = [0.40824829 0.40824829 0.41573971]\n",
      "\n",
      "[[0.5        0.         1.        ]\n",
      " [1.         0.5        0.33333333]\n",
      " [0.         1.         0.        ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[ 1., -1., 2.],\n",
    "[ 2., 0., 0.],\n",
    "[ 0., 1., -1.]])\n",
    "print(data)\n",
    "print(\"\")\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaler.fit(data)\n",
    "scaled_data = scaler.transform(data)\n",
    "\n",
    "print(f\"mean = {scaled_data.mean(axis=0)}, std = {scaled_data.std(axis=0)}\")\n",
    "print(\"\")\n",
    "\n",
    "print(scaled_data)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. RobutScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 아웃라이어의 영향을 최소화한 기법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1. -1.  2.]\n",
      " [ 2.  0.  0.]\n",
      " [ 0.  1. -1.]]\n",
      "\n",
      "mean = [0.         0.         0.22222222], std = [0.81649658 0.81649658 0.83147942]\n",
      "\n",
      "[[ 0.         -1.          1.33333333]\n",
      " [ 1.          0.          0.        ]\n",
      " [-1.          1.         -0.66666667]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[ 1., -1., 2.],\n",
    "[ 2., 0., 0.],\n",
    "[ 0., 1., -1.]])\n",
    "print(data)\n",
    "print(\"\")\n",
    "\n",
    "scaler = preprocessing.RobustScaler()\n",
    "scaler.fit(data)\n",
    "scaled_data = scaler.transform(data)\n",
    "print(f\"mean = {scaled_data.mean(axis=0)}, std = {scaled_data.std(axis=0)}\")\n",
    "print(\"\")\n",
    "\n",
    "print(scaled_data)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## metrics 모듈 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 일반적으로 R**2로 표시되는 계수(즉, y의 분산비율)를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_true = [0, 1, 2, 3]\n",
    "y_pred = [0, 2, 1, 3]\n",
    "score = r2_score(y_true, y_pred)\n",
    "\n",
    "print(score) # 0.6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. explained_variance_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 일반적으로 분산의 차이를 계산(r2_score와 비슷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import explained_variance_score\n",
    "\n",
    "y_true = [0, 1, 2, 3]\n",
    "y_pred = [0, 2, 1, 3]\n",
    "score = explained_variance_score(y_true, y_pred)\n",
    "print(score) # 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  분류의 정확도를 계산, 예측된 값 y_pred가 y_true와 정확히 일치하는 개수의 비율 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_true = [0, 1, 2, 3]\n",
    "y_pred = [0, 2, 1, 3]\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(accuracy) # 0.5\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred, normalize=False)\n",
    "print(accuracy) # 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipeline 모듈 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Pipeline(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### steps: (key, value)를 요소로 하는 리스트를 지정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('reduce_dim', PCA()), ('clf', SVC())])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pipe = Pipeline(steps=[('reduce_dim', PCA()), ('clf', SVC())])\n",
    "print(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. make_pipeline(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  steps: 각 단계를 수행하는 객체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('pca', PCA()), ('svc', SVC())])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pipe = make_pipeline(PCA(), SVC())\n",
    "print(pipe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "e830bd47f5005a1920e1b1ec2d6968b0df80bea4a16275604f69f153be9c7bfc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
